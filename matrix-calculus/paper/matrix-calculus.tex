%Matrix Calculus
%HU, Pili
%Craete: 20120330

%Modify the relative path accordingly
\input{../../common/common.tex}

%This usually doesn't need modification 
\author{HU, Pili\thanks{hupili [at] ie [dot] cuhk [dot] edu [dot] hk}}

%Modify them accordingly===
\title{Matrix Calculus: \\ Derivation and Simple Application}
\date{March 30, 2012\thanks{Last compile:\today}}

\begin{document}

\maketitle
%>============================================
\begin{abstract}
	Matrix Calculus\cite{wiki_mc} is a very useful tool in many 
	engineering problems. Basic rules of matrix calculus are 
	nothing more than ordinary calculus rules covered in 
	undergraduate courses. However, using matrix calculus, 
	the derivation process is more compact. This document is 
	adapted from the notes of a course the author recently attends.
	It builds matrix calculus from scratch. Only prerequisites 
	are basic calculus notions and linear algebra operation.  
	
	To get a quick executive guide, please refer to the cheat 
	sheet in section(\ref{sec:cheat}). 
	
	To see how matrix calculus simplify the process of derivation, please 
	refer to the application in section(\ref{sec:lse}). 
\end{abstract}
%<=======Abstract ENd=========================

%>============================================
\pagebreak
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
%<=======TOC ENd==============================

\section{Introductory Example}

We start with an one variable linear function:
\begin{equation}
	f(x) = ax
\end{equation}

To be coherent, we abuse the partial derivative notation:
\begin{equation}
	\pdiff{f}{x} = a
	\label{eq:fax-single}
\end{equation}

Extending this function to be multivariate, we have:
\begin{equation}
	f(x) = \sum_{i}{a_ix_i} = \tran{a}x
\end{equation}
%The followings are the suggested transpose online? 
%What do you prefer? 
%I choose \mathrm at present. It looks reasonably good. 
%	a^\intercal
%	a^\mathsf{T}
%	a^\mathrm{T}
%	a^\top
%	a^\bot
Where $a = \tran{[a_1,a_2,\ldots,a_n]}$ and 
$x = \tran{[x_1,x_2,\ldots,x_n]}$. 
We first compute partial derivatives directly:
\begin{equation}
	\pdiff{f}{x_k} = \pdiff{(\sum_{i}{a_ix_i})}{x_k} = a_k 
\end{equation}
for all $k=1,2, \ldots, n$. Then we organize $n$ partial derivatives
in the following way:
\begin{equation}
	\pdiff{f}{x} = \left[
	\begin{matrix}
		\pdiff{f}{x_1} \\
		\pdiff{f}{x_2} \\
		\vdots \\
		\pdiff{f}{x_n}
	\end{matrix}
	\right]
	= \left[
	\begin{matrix}
		a_1 \\
		a_2 \\
		\vdots \\
		a_n
	\end{matrix}
	\right]
	= a
	\label{eq:fax-multi}
\end{equation}
The first equality is by proper definition and the rest roots from 
ordinary calculus rules. 

Eqn(\ref{eq:fax-multi}) is analogous to eqn(\ref{eq:fax-single}), except
the variable changes from a scalar to a vector. Thus we want to directly 
claim the result of eqn(\ref{eq:fax-multi}) without those intermediate steps 
solving for partial derivatives separately. Actually, we'll see soon 
that eqn(\ref{eq:fax-multi}) plays a core role in matrix calculus. 

Following sections are organized as follows:
\begin{itemize}
	\item Section(\ref{sec:derivation}) builds commonly used 
	matrix calculus rules from ordinary calculus and linear 
	algebra. Necessary and important properties of linear 
	algebra is also proved along the way. This section is not 
	organized afterhand. All results are proved when we need them. 
	\item Section(\ref{sec:application}) shows some applications 
	using matrix calculus. Table(\ref{tbl:relation}) shows the relation
	between Section(\ref{sec:derivation}) and Section(\ref{sec:application}). 
	\item Section(\ref{sec:cheat}) concludes a cheat sheet of 
	matrix calculus. Note that this cheat sheet may be different 
	from others. Users need to figure out some basic definitions 
	before applying the rules. 
\end{itemize}

\begin{table}[htb]
	\label{tbl:relation}
	\centering
	\caption{Derivation and Application Correspondance}
	\begin{tabular}{c|c}
		\hline
		Derivation & Application \\
		2.1-2.7 & 3.1 \\
		2.9,2.10 & 3.2 \\
		2.8,2.11 & 3.3 \\
		\hline
	\end{tabular}
\end{table}


\section{Derivation}
\label{sec:derivation}

\subsection{Organization of Elements}
From the introductary example, we already see that matrix calculus 
does not distinguish from ordinary calculus by fundamental rules. 
However, with better organization of elements and 
proving useful properties, we can simplify the derivation process 
in real problems. 

The author would like to adopt the following definition:
\begin{mydef}
	\label{def:org}
	For a scalar valued function $f(x)$, the result $\pdiff{f}{x}$
	has the same size with $x$. That is 
	\begin{equation}
		\pdiff{f}{x} = 
		\left[
		\begin{matrix}
			\pdiff{f}{x_{11}} & \pdiff{f}{x_{12}} & \ldots &\pdiff{f}{x_{1n}}\\
			\pdiff{f}{x_{21}} & \pdiff{f}{x_{22}} & \ldots &\pdiff{f}{x_{2n}}\\
			\vdots & \vdots & \ddots & \vdots \\
			\pdiff{f}{x_{m1}} & \pdiff{f}{x_{m2}} & \ldots &\pdiff{f}{x_{mn}}\\
		\end{matrix}
		\right]
	\end{equation}
\end{mydef}

In eqn(\ref{eq:fax-single}), $x$ is a 1-by-1 matrix and the result
$\pdiff{f}{x}=a$ is also a 1-by-1 matrix. In eqn(\ref{eq:fax-multi}), 
$x$ is a column vector(known as n-by-1 matrix) and the result
$\pdiff{f}{x}=a$ has the same size. 

\begin{myex}
\label{ex:tran_x}
By this definition, we have:
\begin{equation}
	\pdiff{f}{\tran{x}} = \tran{(\pdiff{f}{x})} = \tran{a}
\end{equation}
Note that we only use the organization definition in this example. 
Later we'll show that with some matrix properties, this formula 
can be derived without using $\pdiff{f}{x}$ as a bridge. 
\end{myex}

\subsection{Deal with Inner Product}

\begin{mythm}
	\label{thm:inner_product}
	If there's a multivariate scalar function $f(x) = \tran{a}x$, 
	we have $\pdiff{f}{x} = a$.
\end{mythm}

\begin{proof}
See introductary example. 
\end{proof}

Since $\tran{a}x$ is scalar, we can write it equivalently as the 
trace of its own. Thus, 
\begin{myprop}
	\label{prop:trace_inner_product}
	If there's a multivariate scalar function $f(x) = \tr{\tran{a}x}$, 
	we have $\pdiff{f}{x} = a$.
\end{myprop}

$\tr{\bullet}$ is the operator to sum up diagonal elements of a matrix. 
In the next section, we'll explore more properties of trace. 
As long as we can transform our target function into the form of 
theorem(\ref{thm:inner_product}) or proposition(\ref{prop:trace_inner_product}), 
the result can be written out directly. Notice in 
proposition(\ref{prop:trace_inner_product}), $a$ and $x$ are both vectors. 
We'll show later as long as their sizes agree, it holds for matrix $a$ and $x$. 

\subsection{Properties of Trace}

\begin{mydef}
\label{def:trace}
Trace of square matrix is defined as: $\tr{A} = \sum_{i}{A_{ii}}$
\end{mydef}

\begin{myex}
	Using definition(\ref{def:org},\ref{def:trace}), it is very easy to show:
	\begin{equation}
		\pdiff{\tr{A}}{A} = I
	\end{equation}
	since only diagonal elements are kept by the trace operator. 
\end{myex}

\begin{mythm}
\label{thm:trace_prop}
Matrix trace has the following properties:
\begin{itemize}
	\item (1) $\tr{A+B} = \tr{A} + \tr{B}$
	\item (2) $\tr{cA} = c \tr{A}$
	\item (3) $\tr{AB} = \tr{BA}$
	\item (4) $\tr{A_1A_2 \ldots A_n}=\tr{A_nA_1 \ldots A_{n-1}}$
	\item (5) $\tr{\tran{A}B} = \sum_{i}\sum_{j}{A_{ij}B_{ij}}$
	\item (6) $\tr{A} = \tr{\tran{A}}$
\end{itemize}
where $A,B$ are matrices with proper sizes, and $c$ is a scalar value. 
\end{mythm}

\begin{proof}
See wikipedia \cite{wiki_trace} for the proof. 
\end{proof}

Here we explain the intuitions behind each property
to make it easier to remenber. Property(1)
and property(2) shows the linearity of trace. 
Property(3) means two matrices' multiplication inside a 
the trace operator is commutative. Note that the matrix 
multiplication without trace is not commutative and 
the commutative property inside the trace does not hold 
for more than 2 matrices. Property (4) is the proposition of 
property (3) by considering $A_1A_2 \ldots A_{n-1}$ 
as a whole. It is known as cyclic property, so that you can 
rotate the matrices inside a trace operator. Property (5) 
shows a way to express the sum of element by element product using 
matrix product and trace. Note that inner product of two vectors 
is also the sum of element by element product. Property (5) 
resembles the vector inner product by form($\tran{A}B$). 
The author regards property (5) as the extension of inner product 
to matrices(Generalized Inner Product). 

\subsection{Deal with Generalized Inner Product}

\begin{mythm}
	\label{thm:gip}
	If there's a multivariate scalar function $f(x) = \tr{\tran{A}x}$, 
	we have $\pdiff{f}{x} = A$. ($A,x$ can be matrices). 
\end{mythm}

\begin{proof}
	Using property (5) of trace, we can write $f$ as:
	\begin{equation}
		f(x) = \tr{\tran{A}x} = \sum_{ij}A_{ij}x_{ij}
	\end{equation}
	It's easy to show:
	\begin{equation}
		\pdiff{f}{x_{ij}} = \pdiff{(\sum_{ij}A_{ij}x_{ij})}{x_{ij}} = A_{ij}
	\end{equation}
	Organize elements using definition(\ref{def:org}), it is proved. 
\end{proof}

With this theorem and properties of trace we revisit example(\ref{ex:tran_x}). 
\begin{myex}
	For vector $a,x$ and function $f(x) = \tran{a}x$
	\begin{eqnarray}
	 &&\pdiff{f}{\tran{x}} \\
		&=& \pdiff{( \tran{a}x )}{\tran{x}} \\
	\text{($f$ is scalar)}	&=& \pdiff{( \tr{\tran{a}x} )}{\tran{x}} \\
	\text{(property(3))}	&=& \pdiff{( \tr{x\tran{a}} )}{\tran{x}} \\
	\text{(property(6))}	&=& \pdiff{( \tr{a\tran{x}} )}{\tran{x}} \\
	\text{(property of transpose)}	&=& \pdiff{( \tr{\tran{(\tran{a})}\tran{x}} )}{\tran{x}} \\
	\text{(theorem(\ref{thm:gip}))}	&=& \tran{a}
	\end{eqnarray}
	The result is the same with example(\ref{ex:tran_x}), 
	where we used the basic definition. 
\end{myex}

The above example actually demonstrates the usual way of handling a 
matrix derivative problem. 

\subsection{Define Matrix Differential}

Although we want matrix derivative at most time, it turns out 
matrix differential is easier to operate due to the 
form invariance property of differential. Matrix differential 
inherit this property as a natural consequence of the 
following definition. 

\begin{mydef}
	\label{def:mdiff}
	Define matrix differential:
	\begin{equation}
		\dif A = \left[
		\begin{matrix}
			\dif A_{11} & \dif A_{12} & \ldots &\dif A_{1n}\\
			\dif A_{21} & \dif A_{22} & \ldots &\dif A_{2n}\\
			\vdots & \vdots & \ddots & \vdots \\
			\dif A_{m1} & \dif A_{m2} & \ldots &\dif A_{mn}\\
		\end{matrix}
		\right]
	\end{equation}	 
\end{mydef}

\begin{mythm}
Differential operator is distributive through trace operator:
$\dif \tr{A} = \tr{\dif A}$
\end{mythm}

\begin{proof}
	\begin{eqnarray}
		\text{LHS} &=& \dif (\sum_{i}{A_{ii}}) = \sum_{i}{\dif A_{ii}}\\
		\text{RHS} &=&  \tr{
			\begin{matrix}
			\dif A_{11} & \dif A_{12} & \ldots &\dif A_{1n}\\
			\dif A_{21} & \dif A_{22} & \ldots &\dif A_{2n}\\
			\vdots & \vdots & \ddots & \vdots \\
			\dif A_{m1} & \dif A_{m2} & \ldots &\dif A_{mn}\\
			\end{matrix}
		} \\
		&=& \sum_{i}{\dif A_{ii}} = \text{LHS} 
	\end{eqnarray}
\end{proof}

Now that matrix differential is well defined, we want to relate it 
back to matrix derivative. The scalar version differential and 
derivative can be related as follows:
\begin{equation}
	\dif f = \pdiff{f}{x} \dif x
	\label{eq:scalar_dif_der}
\end{equation}
So far, we're dealing with scalar function $f$ and matrix variable $x$. 
$\pdiff{f}{x}$ and $\dif x$ are both matrix according to definition.
In order to make the quantities in eqn(\ref{eq:scalar_dif_der}) equal, 
we must figure out a way to make the RHS a scalar. It's not surprising
that trace is what we want. 

\begin{mythm}
	\label{thm:matrix_dif_der}
	\begin{equation}
		\dif f = \tr{\tran{(\pdiff{f}{x})} \dif x}
		\label{eq:matrix_dif_der}
	\end{equation}
	for scalar function $f$ and arbitrarily sized $x$. 
\end{mythm}

\begin{proof}
	\begin{eqnarray}
		\text{LHS} &=& \dif f \\
		\text{(definition of scalar differential)}
		&=& \sum_{ij}{\pdiff{f}{x_{ij}}\dif x_{ij}} \\
		\text{RHS} &=&  \tr{\tran{(\pdiff{f}{x})} \dif x} \\
		\text{(trace property (5))}&=& \sum_{ij}(\pdiff{f}{x})_{ij} (\dif x)_{ij} \\
		\text{(definition(\ref{def:mdiff}))} &=& \sum_{ij}(\pdiff{f}{x})_{ij} \dif x_{ij} \\
		\text{(definition(\ref{def:org}))} &=& \sum_{ij}\pdiff{f}{x_{ij}} \dif x_{ij} \\
		&=& \text{LHS}
	\end{eqnarray}
\end{proof}

Theorem(\ref{thm:matrix_dif_der}) is the bridge between matrix 
derivative and matrix differential. We'll see in later applications 
that matrix differential is more convenient to manipulate. 
After certain manipulation we can get the form of theorem(\ref{thm:matrix_dif_der}). 
Then we can directly write out matrix derivative using this theorem. 

\subsection{Matrix Differential Properties}

\begin{mythm}
	\label{thm:mdiff_prop}
	We claim the following properties of matrix differential:
	\begin{itemize}
		\item $\dif (cA) = c \dif A$
		\item $\dif (A+B) = \dif A + \dif B$
		\item $\dif (AB) = \dif A B + A \dif B$
	\end{itemize}
\end{mythm}

\begin{proof}
	They're all natural consequences given the definition(\ref{def:mdiff}). 
	We only show the 3rd one in this document. Note that the equivalence 
	holds if LHS and RHS are equivalent element by element. We consider 
	the (ij)-th element. 
	\begin{eqnarray}
		\text{LHS}_{ij} &=& \dif (\sum_{k}{A_{ik}B_{kj}}) \\
		&=& \sum_{k}{(\dif A_{ik}B_{kj} + A_{ik} \dif B_{kj})} \\
		\text{RHS}_{ij} &=& (\dif A B)_{ij} + (A \dif B)_{ij} \\
		&=& \sum_{k}{\dif A_{ik}B_{kj}} + \sum_{k}{A_{ik}\dif B_{kj}} \\
		&=& \text{LHS}_{ij}
	\end{eqnarray}
\end{proof}

\begin{myex}
	\label{ex:xAx}
	Given the function $f(x) = \tran{x} A x$, where 
	$A$ is square and $x$ is a column vector, we can 
	compute:
	\begin{eqnarray}
		\dif f &=& \dif \tr{ \tran{x} A x } \\
		&=& \tr{\dif(\tran{x} A x)} \\
		&=& \tr{\dif(\tran{x}) A x + \tran{x} \dif(A x)} \\
		&=& \tr{\dif(\tran{x}) A x + \tran{x} \dif A x + \tran{x} A \dif x} \\
		\text{(A is constant)}&=& \tr{\dif \tran{x} A x + \tran{x} A \dif x} \\
		&=& \tr{\dif \tran{x} A x} + \tr{\tran{x} A \dif x} \\
		&=& \tr{\tran{x} \tran{A} \dif x} + \tr{\tran{x} A \dif x} \\
		&=& \tr{\tran{x} \tran{A} \dif x + \tran{x} A \dif x} \\
		&=& \tr{(\tran{x} \tran{A} + \tran{x} A) \dif x}
	\end{eqnarray}
	Using theorem(\ref{thm:matrix_dif_der}), we obtain the derivative:
	\begin{equation}
		\pdiff{f}{x} = \tran{ (\tran{x} \tran{A} + \tran{x} A) }
		= Ax + \tran{A} x
	\end{equation}
	When $A$ is symmetric, it simplifies to:
	\begin{equation}
		\pdiff{f}{x} = 2Ax
	\end{equation}
	Let $A=I$, we have:
	\begin{equation}
		\pdiff{(\tran{x}x)}{x} = 2x
	\end{equation}
\end{myex}

\begin{myex}
	\label{ex:dif_inv}
	For a non-singular square matrix $X$, we have $XX^{-1} = I$. 
	Take matrix differentials at both sides:
	\begin{equation}
		0=\dif I = \dif (XX^{-1}) = \dif X X^{-1} + X \dif(X^{-1})
	\end{equation}
	Rearrange terms:
	\begin{equation}
		\dif(X^{-1})=-X^{-1} \dif X X^{-1}  
\end{equation}	 
\end{myex}

\subsection{Schema of Hanlding Scalar Function}

The above example already demonstrates the general schema. 
Here we conclude the process:
\begin{enumerate}
	\item $\dif f = \dif \tr{f} =\tr{\dif f} $
	\item Apply trace properties(see theorem(\ref{thm:trace_prop}))
	and matrix differential properties(see theorem(\ref{thm:mdiff_prop}))
	to get the following form:
		\begin{equation}
			\dif f = \tr{\tran{A}x}
		\end{equation}
	\item Apply theorem(\ref{thm:matrix_dif_der}) to get:
		\begin{equation}
			\pdiff{f}{x} = A
		\end{equation}
\end{enumerate}

To this point, you can handle many problems. In this schema, 
matrix differential and trace play crucial roles. Later we'll 
deduce some widely used formula to facilitate potential 
applications. As you will see, although we rely on matrix differential 
in the schema, the deduction of certain formula may be more easily done 
using matrix derivatives. 

\subsection{Determinant}
\label{sec:det}

For a background of determinant, please refer to \cite{wiki_det}. 
We first quote some definitions and properties without proof:
\begin{mythm}
	Let $A$ be a square matrix:
	\begin{itemize}
		\item The minor $M_{ij}$ is obtained by remove 
		i-th row and j-th column of $A$ and then take determinant
		of the resulting (n-1) by (n-1) matrix. 
		\item The ij-th cofactor is defined as $C_{ij} = (-1)^{i+j}M_{ij}$. 
		\item If we expand determinant with respect to the i-th row, 
		$\det(A) = \sum_{j}{A_{ij}C_{ij}}$. 
		\item The adjugate of $A$ is defined as 
		$\adj(A)_{ij}=(-1)^{i+j}M_{ji}=C_{ji}$. So 
		$\adj(A) = \tran{C}$
		\item For non-singular matrix $A$, we have:
		$A^{-1} = \frac{\adj(A)}{\det(A)} = \frac{\tran{C}}{\det(A)}$
	\end{itemize}
\end{mythm}

Now we're ready to show the derivative of determinant. Note that determinant
is just a scalar function, so all techniques discussed above is applicable. 
We first write the derivative element by element. Expanding determinant
on the i-th row, we have:
\begin{equation}
	\pdiff{\det(A)}{A_{ij}} = \pdiff{(\sum_{j}{A_{ij}C_{ij}})}{A_{ij}} = C_{ij}
\end{equation}
First equality is from determinant definition and second equality is 
by the observation that only coefficient of $A_{ij}$ is left. Grouping all 
elements using definition(\ref{def:org}), we have:
\begin{equation}
	\pdiff{\det(A)}{A} = C = \tran{\adj(A)}
\end{equation}
If $A$ is non-singular, we have:
\begin{equation}
	\pdiff{\det(A)}{A} = \tran{(\det(A)A^{-1})} =\det(A)\tran{(A^{-1})} 
\end{equation}

Next, we use theorem(\ref{thm:matrix_dif_der}) to give the differential 
relationship:
\begin{eqnarray}
	\dif \det(A) &=& \tr{ \tran{(\pdiff{\det(A)}{A})} \dif A} \\
	&=& \tr{ \tran{( \det(A)\tran{(A^{-1})}  )} \dif A} \\
	&=& \tr{ \det(A)A^{-1}  \dif A}
\end{eqnarray}

In many practical problem, the log determinant is more widely used:
\begin{eqnarray}
	\pdiff{\ln \det(A)}{A} = \frac{1}{\det(A)} \pdiff{\det(A)}{A} = \tran{(A^{-1})}
\end{eqnarray}
The first equality comes from chain rule of ordinary calculus(
$\ln \det(A)$ and $\det(A)$ are both scalars). Similarly, we derive
for differential:
\begin{equation}
	\dif \ln \det(A) = \tr{ A^{-1}  \dif A}
\end{equation}
	

\subsection{Vector Function and Vector Variable}

The above sections show how to deal with scalar functions. 
In order to deal with vector function, we should restrict 
our attention to vector variables. It's no surprising that 
the tractable forms in matrix calculus is so scarse. If we 
allow matrix functions and matrix variables, given the fact 
that fully specification of all partial derivatives calls 
for a tensor, it will be difficult to visualize the result
on a paper. An alternative is to stretch functions and variables
such that they appear as vectors. 

An annoying fact of matrix calculus is that, when you 
try to find reference materials, there are always two kinds 
of people. One group calculates as the transpose of another. 
Many online resources are not coherent, which mislead people. 

We borrow the following definitions of Hessian matrix\cite{wiki_hesse} 
and Jacobian matrix\cite{wiki_jacobian} from Wikipedia: 
\begin{equation}
	H(f) = 
	\left[
	\begin{matrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1\,\partial x_n} \\  \\
\frac{\partial^2 f}{\partial x_2\,\partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2\,\partial x_n} \\  \\
\vdots & \vdots & \ddots & \vdots \\  \\
\frac{\partial^2 f}{\partial x_n\,\partial x_1} & \frac{\partial^2 f}{\partial x_n\,\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
	\end{matrix}
	\right]
\end{equation}

\begin{equation}
	J = 
	\left[
	\begin{matrix}
\dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial y_m}{\partial x_1} & \cdots & \dfrac{\partial y_m}{\partial x_n}
	\end{matrix}
	\right]
\end{equation}

Note two things about second order derivative:
\begin{itemize}
	\item By writting the abbreviation $\frac{\partial^2 f}{\partial x_2\,\partial x_1}$, 
	people mean $\pdiff{}{x_2}\left(\frac{\partial f}{\partial x_1}\right)$ by 
	convention. That is, first take derivative with respect to $x_1$ and 
	then take derivative with respect to $x_2$. 
	\item The Hessian matrix can be regarded as first compute the $\pdiff{f}{\tran{x}}$
	(using definition(\ref{def:org}) to organize), and then compute 
	a vector-function-to-vector-variable derivative
	treating $\pdiff{f}{\tran{x}}$ as the function. 
\end{itemize}

Bearing this in mind, we find Hessian matrix and Jacobian matrix actually have 
contradictory notion of organization. In order to be coherent in 
this document, we adopt the Hessian style. That is, each row
corresponds to a variable, and each column corresponds to a function. 
To be concrete:

\begin{mydef}
\label{def:v2v}
%20120402====hpl====
%when I first write this, I think letting f be a row vector 
%looks more intuitive in the following definition. 
%However, the theorem to bridge derivative and differential 
%looks a disaster. So I turn back to modify f to be a column vector. 
%I think it's OK to define in any way, as long as all details 
%are claimed clearly in the context. 
%This again tells us, most time, definition comes the first but 
%it may be the last thing the inventor finalize. 
%We always review from the basic step to see how theorems 
%and propositions can be simplified. 
%The fun of exploring is all gone, when a text-book like document
%is organized. 
For a vector function $f = \tran{[f_1, f_2, \ldots, f_n]}$, and 
$f_i=f_i(x)$ where $x=\tran{[x_1, x_2, \ldots, x_m]}$, 
we have the following definition:
\begin{equation}
	\pdiff{f}{x} = \left[
	\begin{matrix}
			\pdiff{f_1}{x_{1}} & \pdiff{f_2}{x_{1}} & \ldots &\pdiff{f_n}{x_{1}}\\
			\pdiff{f_1}{x_{2}} & \pdiff{f_2}{x_{2}} & \ldots &\pdiff{f_n}{x_{2}}\\
			\vdots & \vdots & \ddots & \vdots \\
			\pdiff{f_1}{x_{m}} & \pdiff{f_2}{x_{m}} & \ldots &\pdiff{f_n}{x_{m}}\\
	\end{matrix}
	\right]
\end{equation}
\end{mydef}

\begin{myex}
According to definition(\ref{def:v2v}), we revisit the definition of Hessian 
and Jacobian. 

Given twice differentiable function $f(x)$, the Hessian is defined as:
\begin{equation}
	H(f) = \pdiff{}{x}\left(\pdiff{f}{\tran{x}}\right)	
\end{equation}

Given two variables $x$ and $y$, if we want to transform $x$ into $y$, 
the Jacobian is defined as:
\begin{equation}
	J = \det \left(\tran{(\pdiff{x}{y})}\right) = 
	\det \left(\pdiff{x}{y}\right)
\end{equation}
Note "Jacobian" is the shorthand name of "Jacobian determinant", 
which is the determinant of "Jacobian matrix". Due to the transpose 
invariance of determinant, the second equality shows that it does not
matter which organization method we use if we only want to do 
compute the Jacobian, rather than Jacobin matrix. However, 
if we're to write out the Jacobin matrix, this may be a pitfall 
depending on what organization of vector-to-vector derivative we define. 
\end{myex}

\subsection{Vector Function Differential}
In the previous sections, we relate matrix differential 
with matrix derivatvie using theorem(\ref{thm:matrix_dif_der}).
This theorem bridges the two quantities using trace. 
Thus we can handle the problem using preferred form 
intechangeably. (As is seen: calculating derivative 
of determinant is more direct; calculating differential 
of inverse is tractable.)

In this section, we provide another theorem to relate 
vector function differential to vector-to-vector 
derivatives. Amazingly, it takes a cleaner form. 

\begin{mythm}
	\label{thm:v2v_dif_der}
	Consider the definition(\ref{def:v2v}), we have: 
	$\dif f  = \tran{(\pdiff{f}{x})}\dif x$
\end{mythm}

\begin{proof}
	Apparently, $\dif f$ has $n$ components, so we prove 
	element by element. Consider the j-th component: 
	\begin{eqnarray}
		\text{LHS}_j &=& \dif f_j \\
		&=& \sum_{i=1}^{m}{\pdiff{f_j}{x_i}\dif x_i}\\
		\text{RHS}_j &=& (\tran{(\pdiff{f}{x})}x)_{j} \\
		&=& \sum_{i=1}^{m}\tran{(\pdiff{f}{x})}_{ji}x_i \\
		&=& \sum_{i=1}^{m}(\pdiff{f}{x})_{ij}x_i \\
		&=& \sum_{i=1}^{m}(\pdiff{f}{x})_{ij}x_i \\
		&=& \sum_{i=1}^{m}(\pdiff{f_j}{x_i})x_i = \text{LHS}_j 
	\end{eqnarray}
\end{proof}

Note that the trace operator is gone compared with theorem(\ref{thm:matrix_dif_der})
due to the nice way of defining matrix vector multiplication. 
We can have a similar schema of handling vector-to-vector derivatives
using this scheme. We don't bother to list the schema again.
Instead, we provide an example. 

\begin{myex}
	\label{ex:gaussian_trans}
	Consider the variable transformation:$x = \sigma \Lambda^{-0.5}\tran{W} \xi$, 
	where $\sigma$ is a real value, $\Lambda$ is full rank diagonal matrix, 
	and $W$ is orthonormal square matrix
	($namely W\tran{W}=\tran{W}W = I$). Compute
	the absolute value of Jacobian.  
	
	First, we want to find $\pdiff{x}{\xi}$. This can be easily done 
	by computing the differential:
	\begin{equation}
		\dif x = \dif (\sigma \Lambda^{-0.5}\tran{W} \xi) 
		= \sigma \Lambda^{-0.5}\tran{W} \dif \xi
	\end{equation}
	Applying theorem(\ref{thm:v2v_dif_der}), we have:
	\begin{equation}
		\pdiff{x}{\xi} = \tran{ (\sigma \Lambda^{-0.5}\tran{W}) }
	\end{equation}
	Thus, 
	\begin{equation}
		J_m = \tran{ (\pdiff{x}{\xi}) } = \tran{(\tran{ (\sigma \Lambda^{-0.5}\tran{W}) })}
		=\sigma \Lambda^{-0.5}\tran{W}
	\end{equation}
	where $J_m$ is the Jacobian matrix and $J= \det(J_m)$. Then we use 
	some property of determinant to calculate the absolute value of 
	Jacobian:
	\begin{eqnarray}
		|J| &=& |\det(J_m)| \\
		&=& \sqrt{|\det(J_m)|\,|\det(J_m)|} \\
		&=& \sqrt{|\det(\tran{J_m})|\,|\det(J_m)|} \\
		&=& \sqrt{|\det(\tran{J_m}J_m)|} \\
		&=& \sqrt{|\det(J_m\tran{J_m})|} \\
		&=& \sqrt{|\det(   W \Lambda^{-0.5}\sigma \sigma \Lambda^{-0.5}\tran{W}  )|} \\
		&=& \sqrt{|\det( \sigma^2  W \Lambda^{-1}\tran{W}  )|}
	\end{eqnarray}
	If the dimension of $x$ and $\xi$ is $d$ and we define $\Sigma = W \Lambda\tran{W}$. 
	A nice result is calculated:
	\begin{equation}
		|J| = \sigma^d \det(\Sigma)^{-1/2}
	\end{equation}
	which we'll see an application of generalizing the multivariate Gaussian distribution. 
\end{myex}

\subsection{Chain Rule}

In the schemas we conclude above, differential is convenient in 
many problems. For derivative, the nice aspect is that we have chain rule, 
which is an analogy to the chain rule in ordinary calculus. However, 
one should be very careful applying this chain rule, for the multiplication 
of matrix requires dimension agreement. 

\begin{mythm}
Suppose we have n column vectors $x^{(1)},x^{(2)},\ldots, x^{(n)}$, each 
is of length $l_1, l_2, \ldots, l_n$. We know $x^{(i)}$ is a function 
of $x^{(i-1)}$, for all $i=2, 3, \dots, n$. The following relationship 
holds:
\begin{equation}
	\pdiff{x^{(n)}}{x^{(1)}} = \pdiff{x^{(2)}}{x^{(1)}}
	\pdiff{x^{(3)}}{x^{(2)}} \ldots \pdiff{x^{(n)}}{x^{(n-1)}}
\end{equation}
\end{mythm}

\begin{proof}
Under definition(\ref{def:v2v}), theorem(\ref{thm:v2v_dif_der}) holds. 
We apply this theorem to each consecutive vectors:
\begin{eqnarray}
	\dif x^{(2)} &=& \tran{(\pdiff{x^{(2)}}{x^{(1)}})} \dif x^{(1)} \\
	\dif x^{(3)} &=& \tran{(\pdiff{x^{(3)}}{x^{(2)}})} \dif x^{(2)} \\
	\ldots \\
	\dif x^{(n)} &=& \tran{(\pdiff{x^{(n)}}{x^{(n-1)}})} \dif x^{(n-1)}
\end{eqnarray} 
Plug previous one into next one, we have:
\begin{eqnarray}
	\dif x^{(n)} &=& \tran{(\pdiff{x^{(n)}}{x^{(n-1)}})} 
	\ldots
	\tran{(\pdiff{x^{(3)}}{x^{(2)}})} 
	\tran{(\pdiff{x^{(2)}}{x^{(1)}})} 
	\dif x^{(1)} \\
	&=& \tran{(\pdiff{x^{(2)}}{x^{(1)}} \pdiff{x^{(3)}}{x^{(2)}}
	\ldots \pdiff{x^{(n)}}{x^{(n-1)}} )} \dif x^{(1)}
\end{eqnarray}
Applying theorem(\ref{thm:v2v_dif_der}) again in the reverse direction, 
we have:
\begin{equation}
	\pdiff{x^{(n)}}{x^{(1)}} = \pdiff{x^{(2)}}{x^{(1)}}
	\pdiff{x^{(3)}}{x^{(2)}} \ldots \pdiff{x^{(n)}}{x^{(n-1)}}
\end{equation}
%Wow, great! this it the most beautiful proof of matrix calculus chain 
%rule I see so far. I'm happy with this discovery when I go out to fetch 
%water in the 9-th floor of SHB. Other proofs can be found online, 
%like \cite{finite_appendix_mc}, which used linear combination of 
%bridge variables beforehand. I think that one is not intuitive, 
%at least for me, who is basically an engineer rather than mathematician. 
%The following proposition is just to show one equation I have doubt 
%when I read \cite{finite_appendix_mc}. They use this one to derive 
%chain rule. However, it is more convenient for me to derive chain rule 
%first, and apply chain rule to get it as a proposition. 
\end{proof}

\begin{myprop}
Consider a chain of: $x$ a scalar, $y$ a column vector, $z$ a scalar. 
$z = z(y), y_i = y_i(x), i=1,2\ldots, n$. Apply the chain rule, we have
	\begin{equation}
		\pdiff{z}{x} = \pdiff{y}{x} \pdiff{z}{y} = 
		\sum_{i=1}^{n}{ \pdiff{y_i}{x} \pdiff{z}{y_i} }
	\end{equation}
\end{myprop}
Now we explain the intuition behind. $x,z$ are both scalar, so 
we're basically calculating the derivative in ordinary calculus. 
Besides, we have a group of "bridge variables", $y_i$. 
$\pdiff{y_i}{x} \pdiff{z}{y_i}$ is just the result of applying 
scalar chain rule on the chain:$x \rightarrow y_i \rightarrow z$. 
The separate results of different bridge variables are additive!
To see why I make this proposition, interested readers can refer 
to the comments in the corresponding \LaTeX ~source file. 

\begin{myex}
	\label{ex:gaussian_der_mu}
	Show the derivative of $\tran{(x-\mu)}\Sigma^{-1}(x-\mu)$ to $\mu$
	(for symmetric $\Sigma^{-1}$). 
	\begin{eqnarray}
		&&\pdiff{[\tran{(x-\mu)}\Sigma^{-1}(x-\mu)]}{\mu}  \\
		&=&\pdiff{[x-\mu]}{\mu}\pdiff{[\tran{(x-\mu)}\Sigma^{-1}(x-\mu)]}{[x-\mu]} \\
	\text{(example(\ref{ex:xAx}))}	&=&\pdiff{[x-\mu]}{\mu}\,2\,\Sigma^{-1}(x-\mu) \\
	(\dif [x-\mu] = -I\, \dif \mu)	&=&-I\,2\,\Sigma^{-1}(x-\mu) \\
		&=&-2\Sigma^{-1}(x-\mu)
\end{eqnarray}	 
\end{myex}

\section{Application}
\label{sec:application}

\subsection{The 2nd Induced Norm of Matrix}

The induced norm of matrix is defined as \cite{wiki_norm}:
\begin{equation}
	||A||_p = \max_{x}{\frac{||Ax||_p}{||x||_p}}
\end{equation}
where $||\bullet||_p$ denotes the p-norm of vectors. Now we solve 
for $p=2$. (By default, $||\bullet||$ means $||\bullet||_2$)

The problem can be restated as:
\begin{equation}
	||A||^2 = \max_{x}{\frac{||Ax||^2}{||x||^2}}
\end{equation}
since all quantities involved are non-negative. Then we consider a 
scaling of vector $x' = tx$, thus:
\begin{equation}
	||A||^2 = \max_{x'}{\frac{||Ax'||^2}{||x'||^2}} 
	=\max_{x}{\frac{||tAx||^2}{||tx||^2}}
	=\max_{x}{\frac{t^2||Ax||^2}{t^2||x||^2}}
	=\max_{x}{\frac{||Ax||^2}{||x||^2}}
\end{equation}
This shows the invariance under scaling. Now we can restrict our attention 
to those $x$ with $||x||=1$, and reach the following formulation:
\begin{eqnarray}
	\maximize && f(x) = ||Ax||^2 \\
	s.t. && ||x||^2 = 1
\end{eqnarray}

The standard way to handle this constrained optimization is 
using Lagrange relaxation:
\begin{equation}
	L(x) = f(x) - \lambda (||x||^2 - 1)
\end{equation}
Then we apply the general schema of handling scalar function 
on $L(x)$. First take differential:
\begin{eqnarray}
	\dif L(x) &=& \dif \tr{L(x)} \\
	&=& \tr{ \dif( L(x) ) } \\
	&=& \tr{ \dif( \tran{x}\tran{A}Ax - \lambda(\tran{x}x - 1)) }  \\
	&=& \tr{ 2 \tran{x} \tran{A}A \dif x - \lambda (2 \tran{x} \dif x) } \\
	&=& \tr{ (2 \tran{x} \tran{A}A - 2\lambda \tran{x}) \dif x }
\end{eqnarray}
Next write out derivative:
\begin{equation}
	\pdiff{L}{x} = 2 \tran{A}A x - 2\lambda x
\end{equation}
Let $\pdiff{L}{x} = 0$, we have:
\begin{equation}
	(\tran{A}A) x = \lambda x
\end{equation}
That means x is the eigen vector of $(\tran{A}A)$
(normalized to $||x||=1$), and $\lambda$ 
is corresponding eigen value. We plug this result back to objective 
function:
\begin{equation}
	f(x) = \tran{x}(\tran{A}Ax) =\tran{x} (\lambda x) = \lambda
\end{equation}
which means, to maximize $f(x)$, we should pick the maximum 
eigen value:
\begin{equation}
	||A||^2 = \max f(x) = \lambda_{\max}(\tran{A}A)
\end{equation}
That is:
\begin{equation}
	||A|| = \sqrt{\lambda_{\max}(\tran{A}A)} = \sigma_{\max}(A)
\end{equation}
where $\sigma_{\max}$ denotes the maximum singular value. 
If $A$ is real symmetric, $\sigma_{\max}(A) = \lambda_{\max}(A) $. 

Now we consider a real symmetric $A$ and check whether:
\begin{equation}
	\lambda^2_{\max}(A) = \max_{x}{\frac{||Ax||^2}{||x||^2}}
	= \max_{x}{\frac{\tran{x}\tran{A}Ax}{\tran{x}x}}
\end{equation}

\begin{proof}
Since $\tran{A}A$ is real symmetric, it has an orthnormal basis 
formed by $n$ eigen vectors, $v_1, v_2, \dots, v_n$, with 
eigen values $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n$. We can 
write $x=\sum_{i}{c_iv_i}$, where $c_i=<x,v_i>$. Then, 
\begin{eqnarray}
	&&\frac{\tran{x}\tran{A}Ax}{\tran{x}x} \\
	\text{($v_k$ is an orthnormal set)}&=& \frac{\sum_i \lambda_i c_i^2}{\sum_{i}{c_i^2}} \\
	&\le & \frac{\sum_i \lambda_1 c_i^2}{\sum_{i}{c_i^2}} \\
	&=& \lambda_1
\end{eqnarray}
Now we have proved an upper bound for $||A||^2$. We show this bound is achievable 
by assigning $x=v_1$. 
\end{proof}
	
	
\subsection{General Multivaraite Gaussian Distribution}	

The first time I came across multivariate Gaussian distribution 
is in my sophomore year. However, at that time, the multivariate 
version is optional, and the text book only gives us the formula 
rather than explaining any intuition behind. I had trouble 
remembering the formula, since I don't know why it is the case.

During the Machine Learning course\cite{xu2012-mlt} I take recently, 
the rationale becomes clear to me. We'll start from the basic notion, 
generalize it to multivariate isotropic Gaussian, and then 
use matrix calculus to derive the most general version. 
The following content is adapted from the corresponding course notes
and homework exercises. 

We start from the univariate Gaussian\cite{wiki_gaussian}:
\begin{equation}
	G(x|\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\e{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}
The intuition behind is:
\begin{itemize}
	\item Use one centroid to represent a set of samples
	coming from the same Gaussian mixture, which 
	is denoted by $\mu$. 
	\item Allow the existence of error. We express the reverse notion
	of "error" by "likelihood". We want the density function describing 
	the distribution that the farther away, the less likely a sample is 
	drawn from the mixture. We want this likelihood to decrease 
	in a accelerated manner. Negative exponential is one proper candidate, 
	say  $\exp\{-D(x,\mu)\}$, where $D$ measures the distance between 
	sample and centroid. 
	\item To fit human's intuition, the "best" choice of distance measure 
	is Euclidean distance, since we live in this Euclidean space. So far, 
	we have $\exp\{-(x-\mu)^2\}$
	\item How fast the likelihood decreases should be controlled by a 
	parameter, say $\exp\{\frac{-(x-\mu)^2}{\sigma}\}$. 
	$\sigma$ can also be thought to control the uncertainty. 
\end{itemize}
Now we already get Gaussian distribution. The division by 2 in the exponent 
is only to simplify deductions. Writting $\sigma^2$ instead of $\sigma$ is 
to align with some statistic quantities. The rest term is basically used 
to do normalization, which can be derived by extending the 1-D
Gaussian integral to a 2-D area integral using 
Fubini's thorem\cite{wiki_fubini}. Interested reader can refer to  
\cite{wiki_gaussian_integral}. 
	
Now we're ready to extend the univariate version to multivariate version. 
The above four characteristics appear as simple interpretation to 
everyone who learned Gaussian distribution before. However, they're 
the real "axioms" behind. Now consider an isotropic Gaussian distribution 
and we start with perpendicular axes. The uncertainty along each axis is 
the same, so the distance measure is now $||x-\mu||_2^2=\tran{(x-\mu)}(x-\mu)$. 
The exponent is $\exp\{ -\frac{1}{2\sigma^2} \tran{(x-\mu)}(x-\mu)\}$. 
Integrating over the volume of $x$ can be done by transforming it 
into iterated form using Fubini's theorem. Then we simply apply the method 
that we deal with univariate Gaussian integral. Now we have 
multivariate isotropic Gaussian distribution:
\begin{equation}
	\label{eq:gaussian_iso}
	G(x|\mu, \sigma^2) = \frac{1}{(2\pi)^{d/2}\sigma^d}
	\exp\{ -\frac{1}{2\sigma^2} \tran{(x-\mu)}(x-\mu)\}
\end{equation}
where $d$ is the dimension of $x,\mu$. 

We are just one step away from a general Gaussian. Suppose we 
have a general Gaussian, whose covariance is not isotropic nor 
does it all along the standard Euclidean axes. Denote the sample
from this Gaussian by $\xi$. We can first apply 
a rotation on $\xi$ to bring it back to the standard axes,
which can be done by left multiplying an
orthongonal matrix $W^T$. Then we scale each component of $W\xi$
by different ratio to make them isotropic, which can be done 
by left multiplying a diagonal matrix $\Lambda^{-0.5}$. The 
exponent $-0.5$ is simply to make later discussion convenient. 
Finally, we multiply it by $\sigma$ to be able to control 
the uncertainty at each direction again. The transform is 
given by $x = \sigma \Lambda^{-0.5}\tran{W} \xi$. Plugging
this back to eqn(\ref{eq:gaussian_iso}), we derive the following
exponent:
\begin{eqnarray}
	&&\exp\{ -\frac{1}{2\sigma^2} \tran{(x-\mu)}(x-\mu)\} \\
	&=& \exp\{ -\frac{1}{2\sigma^2} 
	\tran{(\sigma \Lambda^{-0.5}\tran{W} \xi-\mu)}(\sigma \Lambda^{-0.5}\tran{W} \xi-\mu)\} \\
	&=& \exp\{ -\frac{1}{2\sigma^2} 
	\tran{(\xi-\mu_\xi)}\sigma^2W\Lambda^{-1}\tran{W}(\xi-\mu_\xi)\} \\
	&=& \exp\{ -\frac{1}{2} 
	\tran{(\xi-\mu_\xi)}\Sigma^{-1}(\xi-\mu_\xi)\} 
\end{eqnarray}
where we let $\mu_\xi = \E\xi$, and we plugged in the following result:
\begin{equation}
	\mu = \E x = \E[\sigma \Lambda^{-0.5}\tran{W} \xi]
	=\sigma \Lambda^{-0.5}\tran{W} \E \xi
	=\sigma \Lambda^{-0.5}\tran{W} \mu_\xi
\end{equation}
Now we only need to find the normalization factor to make it a 
probability distribution. 
Note eqn(\ref{eq:gaussian_iso}) integrates to 1, which is:
\begin{equation}
	\int G(x|\mu, \sigma^2)\dif x =  \int \frac{1}{(2\pi)^{d/2}\sigma^d}
	\exp\{ -\frac{1}{2\sigma^2} \tran{(x-\mu)}(x-\mu)\} \dif x = 1
\end{equation}
Transforming variable from $x$ to $\xi$ causes a scaling by absolute Jacobian, 
which we already calculated in example(\ref{ex:gaussian_trans}). 
That is:
\begin{eqnarray}
	\int \frac{1}{(2\pi)^{d/2}\sigma^d}
	\exp\{ -\frac{1}{2} \tran{(\xi-\mu_\xi)}\Sigma^{-1}(\xi-\mu_\xi)\} 
	|J| \dif \xi &=& 1 \\
	\int \frac{\sigma^d \det(\Sigma)^{-1/2}}{(2\pi)^{d/2}\sigma^d}
	\exp\{ -\frac{1}{2} \tran{(\xi-\mu_\xi)}\Sigma^{-1}(\xi-\mu_\xi)\} 
	 \dif \xi &=& 1 \\
	 \int \frac{1}{(2\pi)^{d/2}\det(\Sigma)^{1/2}}
	\exp\{ -\frac{1}{2} \tran{(\xi-\mu_\xi)}\Sigma^{-1}(\xi-\mu_\xi)\} 
	 \dif \xi &=& 1  
\end{eqnarray}
The term inside integral is just the general Gaussian density we want 
to find:
\begin{equation}
	G(x|\mu, \Sigma) = \frac{1}{(2\pi)^{d/2}\det(\Sigma)^{1/2}}
	\exp\{ -\frac{1}{2} \tran{(x-\mu)}\Sigma^{-1}(x-\mu)\} 
	\label{eq:gaussian_gen}
\end{equation}


\subsection{Maximum Likelihood Estimation of Gaussian}
Given $N$ samples, $x_t, t=1,2,\dots,N$, independently indentically distributed
that are drawn from the following Gaussian distribution:
\begin{equation}
	G(x_t|\mu, \Sigma) = \frac{1}{(2\pi)^{d/2}\det(\Sigma)^{1/2}}
	\exp\{ -\frac{1}{2} \tran{(x_t-\mu)}\Sigma^{-1}(x_t-\mu)\} 
	\label{eq:gaussian_gen}
\end{equation}
solve the paramters $\theta=\{\mu, \Sigma\}$, that maximize:
\begin{equation}
	p(X|\theta) = \prod_{t=1}^{N} G(x_t|\theta) 
\end{equation}

It's more conveninent to handle the log likelihood as defined below:
\begin{equation}
	L = \ln p(X|\theta) = \sum_{t=1}^{N} \ln G(x_t|\theta) 
\end{equation}
We write each term of $L$ out to facilitate further processing:
\begin{equation}
	L 
	= -\frac{Nd}{2}\ln (2\pi) - \frac{N}{2}\ln |\Sigma |
	- \frac{1}{2} \sum_t {\tran{(x_t-\mu)}\Sigma^{-1}(x_t-\mu)}
\end{equation}
Taking derivative of $\mu$, the first two terms are gone and 
the third term is already handled by example(\ref{ex:gaussian_der_mu}):
\begin{equation}
	\pdiff{L}{\mu} = \sum_t\Sigma^{-1}(x_t-\mu)
\end{equation} 
Let $\pdiff{L}{\mu}=0$, we solve for $\mu = \frac{1}{N}\sum_t{x_t}$. 
Then take derivative of $\Sigma$. It eaiser to be handled using our 
trace schema:
\begin{eqnarray}
	\dif L &=& \dif [-\frac{N}{2}\ln |\Sigma |]+
	\dif [- \frac{1}{2} \sum_t {\tran{(x_t-\mu)}\Sigma^{-1}(x_t-\mu)}] 
\end{eqnarray}
The first term is:
\begin{eqnarray}
	&& \dif [-\frac{N}{2}\ln |\Sigma |] \\
	&=& -\frac{N}{2}\dif \ln |\Sigma | \\
	\text{(section(\ref{sec:det}))}&=& -\frac{N}{2} \tr{\Sigma^{-1}\dif \Sigma} 
\end{eqnarray}
The second term is:
\begin{eqnarray}	
	&&\dif [- \frac{1}{2} \sum_t {\tran{(x_t-\mu)}\Sigma^{-1}(x_t-\mu)}]\\	
	&=&-\frac{1}{2} \dif \tr{\sum_t {\tran{(x_t-\mu)}\Sigma^{-1}(x_t-\mu)}} \\
	&=&-\frac{1}{2} \dif \tr{\sum_t {(x_t-\mu)\tran{(x_t-\mu)}\Sigma^{-1}}} \\
	\text{(example(\ref{ex:dif_inv}))}&=& -\frac{1}{2} \tr{[\sum_t {(x_t-\mu)\tran{(x_t-\mu)}] 
	(-\Sigma^{-1}\dif \Sigma\Sigma^{-1})}} \\
	&=& \frac{1}{2} \tr{\Sigma^{-1}[\sum_t {(x_t-\mu)\tran{(x_t-\mu)}] \Sigma^{-1}\dif \Sigma}} 
\end{eqnarray}
Then we have:
\begin{equation}
	\pdiff{L}{\Sigma} = -\frac{N}{2}\Sigma^{-1}
	+ \frac{1}{2}\Sigma^{-1} [\sum_t {(x_t-\mu)\tran{(x_t-\mu)}}] \Sigma^{-1} 
\end{equation}
Let $\pdiff{L}{\Sigma}=0$, we solve for 
$\Sigma =\frac{1}{N}\sum_t {(x_t-\mu)\tran{(x_t-\mu)}} $. 

\subsection{Least Square Error Inference: a Comparison}
\label{sec:lse}

This application is rather simple. The purpose is
to compare the derivation with and without matrix 
calculus. 

\subsubsection{Problem Definition}

Here we consider a simple version:
\footnote{Sid Jaggi, Dec 2011, Final Exam Q1 of CUHK ERG2013}

Given the linear system with noise $y=Ax+n$, where $x$ is the input, 
$y$ is the output and $n$ is the noise term. $A$ as the system 
parameter is a known $p \times q$ matrix. 
Now we have one observation of output, $\hat{y}$. 
We want to infer the "most possible" corresponding input $\hat{x}$
defined by the following formula:
\footnote{Assuming Gaussian noise, the maximum likelihood inference results
in the same expression of least squares. For simplicity, we ignore the argument
of this conclusion. Readers can refer to the previous example to 
see the relation between least squares and Gaussian distribution.}
\begin{equation}
	\hat{x} = \argmin_{x:\hat{y}=Ax+n}{||n||^2}
\end{equation}

\subsubsection{Ordinary Formulation and Derivation}

First, we write the error function explicitly:
\begin{eqnarray}
	f(x) &=& ||\hat{y} - Ax||^2 \\
	&=& \sum_{i=1}^{p}{(\hat{y}_i - (Ax)_i)^2} \\
	&=& \sum_{i=1}^{p}{(\hat{y}_i - \sum_{j=1}^{q}{A_{ij}x_j} )^2}
\end{eqnarray}

This is the quadratic form of all $x_k, k=1,2 \ldots q$. We take 
derivative of $x_k$:
\begin{eqnarray}
	\pdiff{f}{x_k} 
	&=& \sum_{i=1}^{p}
		{[2(\hat{y}_i - \sum_{j=1}^{q}{A_{ij}x_j} )
		(- A_{ik})]} \\
	&=& -2 \sum_{i=1}^{p}
		{[(\hat{y}_i - (Ax)_i )
		A_{ik}]} \\
	&=& -2 <(\hat{y} - (Ax)), A_{k}> \\
	&=& -2 \tran{A_k}(\hat{y} - Ax)
%fault derivation below. 
%	&=& \sum_{i=1}^{p}
%		{[2(\hat{y}_i - \sum_{j=1}^{q}{A_{ij}x_j} )
%		\hat{y_i}]}
%		- \sum_{i=1}^{p}
%		{[2(\hat{y}_i - \sum_{j=1}^{q}{A_{ij}x_j} )
%		A_{ik}]} \\
%	&=& \sum_{i=1}^{p}
%		{[2 (\hat{y}_i - (Ax)_i)\hat{y_i} ]} 
\end{eqnarray}
where $A_k$ is the k-th column of $A$ and $<.,.>$ denotes the inner product 
of two vectors. 
We let all $\pdiff{f}{x_k} = 0$ and put the $q$ equations together
to get the matrix form:
\begin{equation}
	\left[
	\begin{matrix}
		\pdiff{f}{x_1} \\
		\pdiff{f}{x_2} \\
		\vdots \\
		\pdiff{f}{x_q}
	\end{matrix}
	\right]
	=\left[
	\begin{matrix}
		-2 \tran{A_1}(\hat{y} - Ax) \\
		-2 \tran{A_2}(\hat{y} - Ax) \\
		\vdots \\
		-2 \tran{A_q}(\hat{y} - Ax)
	\end{matrix}
	\right]
	= -2 \tran{A}(\hat{y} - Ax) = 0 
\end{equation}
We solve for:
\begin{equation}
	x = (\tran{A}A)^{-1}\tran{A}\hat{y}
\end{equation}
when $(\tran{A}A)$ is invertible. 


\subsubsection{Matrix Formulation and Derivation}

The error function can be written as:
\begin{eqnarray}
	f(x) &=& ||\hat{y} - Ax||^2 \\
	&=& \tran{(\hat{y} - Ax)}(\hat{y} - Ax) \\
\end{eqnarray}

We apply the trace schema to obtain the differential:
\begin{eqnarray}
	\dif f 
	&=& \dif \tr{ \tran{(\hat{y} - Ax)}(\hat{y} - Ax) } \\
	&=& \tr{ \dif \tran{(\hat{y} - Ax)}(\hat{y} - Ax)
		+ \tran{(\hat{y} - Ax)} \dif (\hat{y} - Ax) } \\
	&=& - \tr{ \tran{(\dif x)} \tran{A} (\hat{y} - Ax)
		+ \tran{(\hat{y} - Ax)} A \dif x} \\
	&=& - \tr{ \tran{(\hat{y} - Ax)} A \dif x
		+ \tran{(\hat{y} - Ax)} A \dif x} \\
	&=& -2 \tr{ \tran{(\hat{y} - Ax)} A \dif x} 
\end{eqnarray}
The derivative is:
\begin{equation}
	\pdiff{f}{x} = -2 \tran{A}(\hat{y} - Ax)
\end{equation}
We let $\pdiff{f}{x}=0$ and get the same result. 

\subsubsection{Remarks}

Without matrix calculus, we can still achieve the 
goal. Every step is straightforward using ordinary 
calculus, except for the last one where we organize 
everything into the matrix form. 

With matrix calculus, we utilize those well established 
matrix operators, like matrix multiplication. Thus
the notation is sharply simplified. Comparing the two 
derivations, we find the one with matrix calculus 
is absent of annoying summations. This should be very 
advantegeous in more complex expressions. 

This example aims to disclose the essence of matrix calculus.
Whenever we need new rules, it's always workable in 
traditional calculus way. 

\section{Cheat Sheet}
\label{sec:cheat}

This section is not yet available. I plan to sum up later. By the time 
I initiate this document, I had three passes of matrix calculus
(appendix). The 4-th 
pass is left for me to refresh the mind in some future day. 

Nevertheless, I think I already covered essential schema, property, 
formula, etc. Not all of them are in the form of theorems or propositions. 
A large part is covered using examples. Putting them all in one paper 
may be a good cheat sheet. 

BTW, I also advocates version control and online 
collaboration tools for academic use. Such great platform 
should not be enjoyed by mere IT industry. I'll 
be glad if someone can contribute this section. Learning 
to use github costs no more than one hour. I'll of course 
pull any reasonable modifications from anyone. 
Typos, suggestions, technical fix, etc, will be acknowledged 
in the following section. 

%>============================================
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
Thanks prof. XU, Lei's tutorial on matrix calculus\cite{xu2012-mlt}. 
Besides, the author also benefits a lot from other online 
materials. 
%<=======Acknowledgements ENd=================

%>============================================
\addcontentsline{toc}{section}{References}
\input{../reference/gen_bib.bbl}
%<=======Bibliography ENd=====================

%>============================================
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\subsection*{Message from Initiator}

My three passes so far:
\begin{enumerate}
	\item Learn the trace schema in the lecture. 
	\item Organize notes when doing the first homework. 
	\item In mid-term review, I revisited most properties. 
\end{enumerate}
Not until finishing the current version of this document do 
I grasp some details. It's very likely that everything is 
smooth when you read it. However, it may become a problem
repeating some derivations. Those definitions I adopted 
are not all the initial version. Sometimes, I find it ugly 
to express certain result. Then I turn back modify the definition. 
Anyway, I don't see a unified dominant definition. If you check 
the history and talk page of Wikipedia\cite{wiki_mc}, you'll find editing-wars
happen every now and then. Even the single matrix calculus page
is not coherent\cite{wiki_mc}, let alone other pages. A precaution 
is, if you can not figure out the definition from the context, 
you'd better don't trust that formula. The safest thing is to learn
his way of derivation and derive from your own definition. 
I also call for everyone's review of this document. It doesn't matter
what background you have, for we try to build the system from 
scratch. I myself is an engineer rather than mathematician. 
When I first read others work, like \cite{finite_appendix_mc}, 
I bother to repeat the derivations and find some part is 
incorrect. That's why I reorganize my written notes, 
in order for me to reference in the future. 

Shiney is a lazy girl who don't bother to derive
anything but also wish to learn matrix calculus. 
Thus I compose all my notes for her. Besides, she 
wants to learn \LaTeX ~in a fast food fashion. This 
document actually contains most things she needs 
in writing, from which the source and outcome are 
straightforward to learn. 

%<=======Appendix ENd=========================

\end{document}
